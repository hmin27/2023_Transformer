{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import torch\n",
    "\n",
    "en_vocab_file = \"../en.model\"\n",
    "de_vocab_file = \"../de.model\"\n",
    "\n",
    "en_vocab = spm.SentencePieceProcessor()\n",
    "de_vocab = spm.SentencePieceProcessor()\n",
    "\n",
    "en_vocab.load(en_vocab_file)\n",
    "de_vocab.load(de_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab pieces:  ['▁summer', '▁is', '▁le', 'av', 'ning']\n",
      "encode_as_ids:  [3299, 54, 293, 638, 548]\n",
      "vocab pieces:  ['▁winter', '▁is', '▁coming']\n",
      "encode_as_ids:  [5026, 54, 1079]\n",
      "tensor:  tensor([[3299,   54,  293,  638,  548],\n",
      "        [5026,   54, 1079,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "lines = [\n",
    "  \"summer is leavning\",\n",
    "  \"winter is coming\"\n",
    "]\n",
    "\n",
    "inputs = []\n",
    "for line in lines:\n",
    "  pieces = en_vocab.encode_as_pieces(line)\n",
    "  ids = en_vocab.encode_as_ids(line)\n",
    "  inputs.append(torch.tensor(ids))\n",
    "  print('vocab pieces: ', pieces)\n",
    "  print('encode_as_ids: ', ids)\n",
    "\n",
    "# 입력 최대 길이에 맞춰 0으로 패딩\n",
    "inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "\n",
    "\n",
    "print('tensor: ',inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thank you so much, Chris.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train_df.csv')\n",
    "train_df['en'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "\n",
    "class en2deDataset(Dataset):\n",
    "    def __init__(self, dataframe, src_vocab, tgt_vocab, src_lang, tgt_lang):\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.src_lang = src_lang  # 'en'\n",
    "        self.tgt_lang = tgt_lang  # 'de'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_sentence = self.dataframe[self.src_lang][idx]\n",
    "        tgt_sentence = self.dataframe[self.tgt_lang][idx]\n",
    "        \n",
    "        # encode_as_ids: 문장을 입력하면 정수 시퀀스로 변환\n",
    "        scr_enc = self.src_vocab.encode_as_ids(src_sentence)\n",
    "        tgt_enc = self.tgt_vocab.encode_as_ids(src_sentence)\n",
    "\n",
    "        return (torch.tensor(scr_enc), torch.tensor(tgt_enc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "class en2deDataLoader:\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        # zip(*batch) = 배치에 저장된 모든 src와 tgt 나눠서 묶는 용도\n",
    "        # batch = (src_sentence), (tgt_sentence) 쌍\n",
    "        src_batch, tgt_batch = zip(*batch)\n",
    "        src_padded = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "        tgt_padded = torch.nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
    "\n",
    "        return src_padded, tgt_padded\n",
    "\n",
    "    def get_data_loader(self):\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size, collate_fn=self.collate_fn)\n",
    "\n",
    "\n",
    "dataset = en2deDataset(train_df, en_vocab, de_vocab, 'en', 'de')\n",
    "data_loader = en2deDataLoader(dataset, BATCH_SIZE)\n",
    "train_loader = data_loader.get_data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
